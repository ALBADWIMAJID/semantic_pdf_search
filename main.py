# âœ… Ù…Ø´Ø±ÙˆØ¹: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ ÙÙŠ Ù…Ù„ÙØ§Øª PDF Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­Ù„ÙŠ (nomic-embed-text)

import os
import glob
import time
import random
import numpy as np
import pyarrow as pa
from typing import List, Dict
from docling.chunking import HybridChunker
 # type: ignore
from docling.document_converter import DocumentConverter # type: ignore
import lancedb # type: ignore
from lancedb.pydantic import LanceModel # type: ignore
from transformers.tokenization_utils_base import PreTrainedTokenizerBase # type: ignore
from tiktoken import get_encoding
from dotenv import load_dotenv
from IPython.display import display
import ipywidgets as widgets # type: ignore
from langchain.embeddings import OllamaEmbeddings

load_dotenv()

# âœ… Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠ
embedding_model = OllamaEmbeddings(
    model="nomic-embed-text",
    base_url="http://localhost:11434"
)



# âœ… Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙˆÙƒÙ†ÙŠØ²Ø±
class OpenAITokenizerWrapper(PreTrainedTokenizerBase):
    def __init__(self, model_name: str = "cl100k_base", max_length: int = 8191, **kwargs):
        super().__init__(model_max_length=max_length, **kwargs)
        self.tokenizer = get_encoding(model_name)
        self._vocab_size = self.tokenizer.max_token_value

    def tokenize(self, text: str, **kwargs) -> List[str]:
        return [str(t) for t in self.tokenizer.encode(text)]

    def _tokenize(self, text: str) -> List[str]:
        return self.tokenize(text)

    def _convert_token_to_id(self, token: str) -> int:
        return int(token)

    def _convert_id_to_token(self, index: int) -> str:
        return str(index)

    def get_vocab(self) -> Dict[str, int]:
        return dict(enumerate(range(self.vocab_size)))

    @property
    def vocab_size(self) -> int:
        return self._vocab_size

    def save_vocabulary(self, *args):
        return ()

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        return cls()

# âœ… Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø¹Ø§Ù…Ø©
tokenizer = OpenAITokenizerWrapper()
MAX_TOKENS = 8191

# âœ… ØªØ­ÙˆÙŠÙ„ PDF Ø¥Ù„Ù‰ ÙƒØ§Ø¦Ù† Docling
def convert_pdf_to_document(pdf_path):
    converter = DocumentConverter()
    if not pdf_path.startswith('http'):
        pdf_path = os.path.abspath(pdf_path)
    return converter.convert(pdf_path)

# âœ… Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ù…Ù„ÙØ§Øª PDF ÙÙŠ Ù…Ø¬Ù„Ø¯
def process_pdf_documents(pdf_dir):
    pdf_files = glob.glob(os.path.join(pdf_dir, "*.pdf"))
    if not pdf_files:
        print(f"\nâŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª PDF ÙÙŠ: {pdf_dir}")
        return []

    all_chunks = []
    for pdf_file in pdf_files:
        print(f"ğŸ” ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ„Ğ°Ğ¹Ğ»Ğ°: {pdf_file}")
        result = convert_pdf_to_document(pdf_file)
        chunker = HybridChunker(tokenizer=tokenizer, max_tokens=MAX_TOKENS, merge_peers=True)
        chunks = list(chunker.chunk(dl_doc=result.document))
        print(f"âœ… Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¾ {len(chunks)} Ñ‡Ğ°Ğ½ĞºĞ¾Ğ² Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° {pdf_file}")
        all_chunks.extend(chunks)
    return all_chunks

# âœ… Ø¥Ù†Ø´Ø§Ø¡ Ø£Ùˆ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© LanceDB
def create_or_connect_db(db_path="data/lancedb"):
    os.makedirs(os.path.dirname(db_path), exist_ok=True)
    return lancedb.connect(db_path)

# âœ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯ÙˆÙ„ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª
def create_and_fill_table(db, chunks, table_name="pdf_chunks"):
    print(f"ğŸ§  Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ {len(chunks)} Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²...")
    schema = pa.schema([
        pa.field("text", pa.string()),
        pa.field("vector", pa.list_(pa.float32(), 768)),
        pa.field("doc_name", pa.string()),
        pa.field("chunk_id", pa.int32())
    ])
    table = db.create_table(table_name, schema=schema, mode="overwrite")

    processed_chunks = []
    for i, chunk in enumerate(chunks):
        chunk_text = chunk.text if hasattr(chunk, "text") else str(chunk)
        doc_name = chunk.metadata.get("file_name", f"doc_{i//2}") if hasattr(chunk, "metadata") else f"doc_{i//2}"
        processed_chunks.append({"text": chunk_text, "doc_name": doc_name, "chunk_id": i})

    progress = widgets.IntProgress(value=0, min=0, max=len(processed_chunks), description='ĞŸÑ€Ğ¾Ğ³Ñ€ĞµÑÑ:', bar_style='info')
    display(progress)

    for i, chunk in enumerate(processed_chunks):
        try:
            print(f"   ğŸ§® Ğ­Ğ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³ Ñ‡Ğ°Ğ½ĞºĞ° {i+1}/{len(processed_chunks)}")
            vector = embedding_model.embed_query(chunk["text"])
            chunk["vector"] = vector
            table.add([chunk])
            progress.value = i + 1
        except Exception as e:
            print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ‡Ğ°Ğ½ĞºĞ° {i+1}: {e}")

    return table

# âœ… Ø§Ù„Ø¨Ø­Ø« Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¬Ø¯ÙˆÙ„
def search_in_table(query_text, table, limit=3):
    print(f"ğŸ” Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ: '{query_text}'")
    try:
        embedding = embedding_model.embed_query(query_text)
        results = table.search(embedding).limit(limit).to_pandas()
        return results
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ¸ÑĞºĞ°: {e}")
        return None

# âœ… Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
def display_search_results(results):
    if results is None or len(results) == 0:
        print("âŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬.")
        return

    for i, row in results.iterrows():
        print(f"\nğŸ”¹ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ {i+1} â€” Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚: {row['doc_name']} | Ğ§Ğ°Ğ½Ğº ID: {row['chunk_id']}")
        print(f"ğŸ“ {row['text'][:300]}...")

# âœ… ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¯ÙˆØ±Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©
def run_pipeline(pdf_dir="pdfs", db_path="./lancedb", table_name="pdf_docs"):
    print("\n==================================================")
    print(f"ğŸš€ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° PDF: {pdf_dir}")
    chunks = process_pdf_documents(pdf_dir)
    if not chunks:
        return False
    db = create_or_connect_db(db_path)
    table = create_and_fill_table(db, chunks, table_name)
    return True

if __name__ == '__main__':
    success = run_pipeline("pdfs", "./lancedb", "pdf_docs")
    if success:
        db = lancedb.connect("./lancedb")
        table = db.open_table("pdf_docs")
        while True:
            question = input("Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¸Ğ»Ğ¸ 'exit': ")
            if question.lower() == 'exit':
                break
            results = search_in_table(question, table)
            display_search_results(results)
